**跨模态交互更细粒度**

- 除了“先各自编码→拼 token→Transformer”，可以在中间多做几次双模态或三模态交互（比如先 TS↔LAB，再 TS+LAB↔IMG）；
- 引入 **Cross-Attention** 而非单纯的 Self-Attention，在每层做 Query 来自某模态、Key/Value 来自另一模态。

**可学习的模态权重**

- 在融合时给每个模态的 CLS token 加上可训练的标量权重（或门控机制），让模型自己学会“TS、IMG、LAB”哪个样本更重要。

**多尺度特征与残差连接**

- 在 TimeSeries 分支中加入 1D-CNN（不同 kernel、不同分辨率），再将多尺度输出拼到 Transformer token；
- 在 Shared-Transformer 里用 **Adapter** 或 **残差跨层连接**，让低层模态特定信息能更好传到高层。

**高效 Transformer 变体**

- 用 Linformer/Performer/Swin-Transformer 等能够处理更长序列、计算更快的变种。
- 或者尝试 **Routing Transformer**，只在相关 token 之间做 attention，减少冗余。

**多头池化（Multi-token fusion）**

- 不止一个 CLS token，可以使用多个 learnable “query token” 来聚合不同子空间的特征，然后再融合；
- 或者借鉴 Set Transformer，做 **Induced Set-Pooling**。





**不平衡与边缘样本强化**

- 用 **Focal Loss** 或 **Label-Distribution-Aware Margin Loss** 强化难分样本；
- 可以做 **Hard Example Mining**，在训练中动态挑“最难”三模态样本重点学习。











1. **lab分支提前融入进去**
2. Encoder自己写或者换个模型大点的，目前训练出来的模型太小了
3. **Time分支的Encoder改一下，例如GNN，想办法在Time分支讲故事**
4. **Time分支和Image分支对比学习loss，对齐一下两个模态（图对比学习法），对比学习loss和分类loss结合成为最终loss**
5. Image和lab虽然简单但都是创新点，讲好故事
6. Image绘图方式和排布有待尝试验证
7. 看一看投稿期刊上相关方向的论文，参考一下要求
8. 模型图改一下，还有不少地方有问题（Time分支patch embedding，Lab分支的Feature画的有问题，只有一个global token），Time分支输入化成坐标系的样子，Image分支输入直接画成$3 \times 4$的样子
9. 课程设计准备一下