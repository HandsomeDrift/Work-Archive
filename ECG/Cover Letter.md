Dear Editors,

We are pleased to submit our manuscript entitled "GFM-MIP: Graph-informed and FiLM-enhanced Multimodal Fusion for Myocardial Infarction Prediction" for consideration for publication in your esteemed journal.

Rapid and accurate diagnosis of myocardial infarction (MI) remains a crucial clinical challenge. While electrocardiograms (ECGs) serve as frontline diagnostic tools, traditional unimodal approaches frequently overlook vital complementary physiological information, limiting diagnostic accuracy and robustness. To overcome this, we propose GFM-MIP, a novel multimodal framework that integrates 12-lead ECG time-series signals, ECG waveform images, and laboratory biomarkers within a unified deep learning architecture.

Our framework employs a Graphormer encoder to effectively capture spatial and temporal dependencies among ECG leads, while a Vision Transformer extracts critical morphological features from ECG images. A distinctive innovation of our approach is the Feature-wise Linear Modulation (FiLM), which dynamically incorporates patient-specific laboratory biomarkers into both temporal and morphological feature extraction processes. Additionally, a Transformer-based fusion module seamlessly aggregates these diverse modality representations, further reinforced by a contrastive learning objective designed to align signal and image modalities coherently.

We conducted extensive experimental validations using a real-world clinical dataset from the Second Hospital of Shandong University and three public ECG datasets. Results consistently demonstrate that GFM-MIP achieves state-of-the-art performance across multiple key evaluation metrics, outperforming several robust baseline models. Ablation studies further underline the critical contribution of each component and modality in our approach.

Given your journal's strong focus on innovative clinical methodologies and applied artificial intelligence in healthcare, we believe our work aligns perfectly with your readership's interests. Our manuscript is original, has not been published elsewhere, and is not under consideration by any other publication.

Thank you very much for considering our manuscript. We look forward to your feedback and are eager to respond to any suggestions or questions you may have.

Sincerely,

Xiantong Xiang

School of Software, Shandong University
 Jinan, Shandong, China
 Email: [ysgong@sdu.edu.cn](mailto:ysgong@sdu.edu.cn)
 (On behalf of all co-authors)