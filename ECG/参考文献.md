## I. Introduction

Cardiovascular diseases (CVDs) remain a leading cause of mortality worldwide, accounting for approximately one-third of all global deaths. Among these, myocardial infarction (MI), commonly known as a heart attack, poses a significant public health challenge. MI typically results from an obstruction in coronary blood flow, often due to arterial blockage, demanding prompt and accurate diagnosis for effective intervention. Timely detection is crucial not only to reduce mortality but also to mitigate severe complications such as heart failure, ultimately enhancing patient outcomes. The electrocardiogram (ECG) is widely recognized as an essential, non-invasive, cost-effective diagnostic tool due to its real-time monitoring capabilities, accessibility, and reliable diagnostic insights.

Despite its widespread use, conventional ECG analysis methods—particularly those relying on a single modality—exhibit notable limitations. These approaches are susceptible to noise, variability across patients, and often lack comprehensive systemic physiological context. Recent advancements in deep learning have substantially improved ECG classification performance; however, most models typically rely exclusively on either time-series data or static images. This singular focus neglects the potential synergies achievable by integrating multiple data modalities, thereby limiting the models' diagnostic accuracy and interpretability.

Recognizing this gap, multimodal learning has gained increasing attention in medical diagnostics. Laboratory tests, for instance, provide critical biochemical context unattainable through ECG alone, while ECG images convey essential structural and morphological insights that facilitate visual diagnostics. The integration of these complementary modalities promises richer, more informative representations of patient conditions, thus enabling more accurate and personalized clinical evaluations. For example, Toprak et al. [1] demonstrated that deep learning models trained on high-sensitivity cardiac troponin data outperformed rule-based pathways for MI triage, validating the value of laboratory biomarkers in acute cardiovascular diagnostics. Additionally, Kilimci et al. [2] showed that transformer-based models using ECG images can effectively detect heart disease, reinforcing the potential of visual cues in automated diagnosis.

Notably, multimodal methods have shown effectiveness beyond cardiology. Tang et al. [3] introduced a meta-learning ECG-language model for few-shot clinical question answering, highlighting how cross-modal fusion improves generalization. In parallel, Holmstrom et al. [4] showed that ECG signals can even reveal non-cardiac comorbidities such as chronic kidney disease, underscoring ECG's latent richness. These insights motivate multimodal approaches that extend beyond waveform analysis.

Motivated by these clinical and methodological developments, we propose **MIRAGE** (Multimodal Integration via Representational Alignment using Graphormer and Encoders), a novel multimodal framework specifically designed for ECG-based cardiovascular disease classification. MIRAGE synergistically integrates three complementary data modalities: 12-lead ECG time-series data representing electrical dynamics, ECG grayscale images highlighting morphological patterns, and laboratory test results reflecting systemic biochemical conditions.

The architectural choices within MIRAGE are intentionally tailored to address distinct clinical challenges. Recognizing that conventional time-series models inadequately capture inter-lead relationships inherent in 12-lead ECG data, we adopt a Graphormer-based encoder. By treating each ECG lead as a graph node, this approach explicitly captures the physiological interdependencies across leads, enhancing diagnostic accuracy and interpretability.

Complementing this, we incorporate a Vision Transformer (ViT) for ECG image analysis. This choice reflects clinical practice, where visual waveform interpretation provides crucial morphological cues. Unlike traditional convolutional methods, ViTs leverage global self-attention mechanisms, allowing superior capture of morphological patterns associated with conditions such as ST-segment elevation [2].

To further enhance the diagnostic power, MIRAGE introduces Feature-wise Linear Modulation (FiLM) layers within both encoding branches. Laboratory biomarkers such as troponin and D-dimer serve as modulation signals, dynamically adjusting feature representations according to individual physiological conditions [1]. This strategy ensures patient-specific, context-aware feature extraction, significantly improving personalized diagnostic assessments.

To comprehensively integrate these diverse modalities, MIRAGE employs a Transformer-based fusion module. This module effectively bridges semantic gaps between modalities, modeling intricate cross-modal interactions among temporal, visual, and biochemical data. Furthermore, a contrastive learning strategy complements the standard classification task, encouraging coherence and alignment across multimodal representations.

In summary, MIRAGE's primary contributions include:

- A dual-branch encoder architecture combining Graphormer and ViT to effectively exploit temporal and spatial ECG data;
- FiLM-based modulation using laboratory biomarkers for personalized, context-rich feature extraction;
- A Transformer-driven fusion strategy that seamlessly integrates multimodal features;
- An integrated contrastive learning module that enhances multimodal representational coherence and alignment.

Through these innovations, MIRAGE addresses key limitations of traditional single-modality ECG approaches, providing a robust, interpretable, and clinically valuable solution for myocardial infarction diagnosis.

## II. Related Work

### A. ECG Classification: Progress and Challenges

Automated electrocardiogram (ECG) classification has been extensively studied due to its critical role in cardiovascular diagnostics. Classical machine learning approaches relied on hand-crafted features and shallow classifiers. With the rise of deep learning, convolutional and recurrent neural networks have dominated the field, achieving high performance on standard datasets such as MIT-BIH and PTB-XL. More recently, transformer-based architectures have demonstrated superior ability to capture global temporal dependencies, further pushing performance boundaries [18].

Nevertheless, most of these models are unimodal, limited either to raw ECG signals or derived images. This unimodal focus often fails to account for the multifaceted nature of clinical diagnosis, which incorporates diverse modalities and biomarkers. For instance, Liu et al. [13] proposed a CNN-based model for detecting acute MI, but lacked biochemical context; Jafarian et al. [14] investigated shallow and deep models for MI localization but relied solely on signal morphology; and Lee et al. [15] used deep learning to screen for low ejection fraction via ECG, yet did not explore multimodal integration. In a broader setting, Kalmady et al. [5] developed deep learning models capable of predicting 15 cardiovascular conditions from over 1.6 million ECGs, demonstrating scalable multi-condition ECG diagnostics.

Recent studies also highlighted ECG's potential in detecting systemic conditions. Holmstrom et al. [4] reported high performance in detecting chronic kidney disease from ECGs, particularly among younger populations. Meanwhile, Khurshid et al. [6] demonstrated that ECG-based deep learning, combined with clinical features, can improve atrial fibrillation risk prediction.

### B. Graph Neural Networks and Transformers in Medical Applications

Graph Neural Networks (GNNs) have demonstrated potential in modeling structured data in medicine. For ECG analysis, GNNs can capture inter-lead relationships more explicitly than sequential models. For example, Backhaus et al. [19] utilized AI to quantify myocardial strain, indicating that spatial dependencies within cardiac features are clinically valuable.

Meanwhile, Vision Transformers (ViT) and time-series transformers have gained traction in healthcare due to their global modeling capabilities. Nie et al. [18] adapted Transformer architectures to long ECG sequences and showed improved forecasting capabilities. Kilimci et al. [2] demonstrated that ViT variants outperform CNNs in ECG image classification tasks. Yet, few works unify GNN and Transformer architectures within a single framework. MIRAGE addresses this by employing a Graphormer-based ECG encoder and a ViT branch for image modeling, jointly learning from structure and morphology.

### C. Multimodal Fusion in Cardiovascular Diagnosis

Multimodal integration holds great promise in enhancing diagnostic reliability by capturing complementary perspectives. Several works have explored combining ECG with clinical parameters. Lampert et al. [20] used deep learning to predict cardiomyopathy in patients with premature ventricular contractions, yet only incorporated limited structured clinical features. Yuan et al. [16] demonstrated the predictive value of ECG for atrial fibrillation but used sinus rhythm data in isolation. Wei et al. [17] proposed a bimodal masked autoencoder (BMIRC) combining time and frequency domain ECG data, introducing internal representation connections to improve cross-modal learning. While effective, BMIRC was limited to two intrinsic signal modalities and did not utilize external clinical context. Toprak et al. [1] further verified the clinical relevance of hs-cTn data in deep learning-based prediction. Boeddinghaus et al. [7] and Doudesis et al. [8] also reported that ML-driven ECG interpretation can outperform conventional rule-based diagnostic pathways in acute MI triage. Al-Zaiti et al. [9,10] demonstrated the importance of combining ECG features with clinical metadata for risk stratification and occlusion MI identification.

MIRAGE extends these approaches by integrating time-series ECG, image ECG, and laboratory test results, and by introducing a patient-conditioned feature modulation mechanism using FiLM. This allows more personalized and explainable representation learning aligned with real-world clinical workflows.

### D. Contrastive Learning for Representation Alignment

Contrastive learning methods have shown remarkable ability to align multimodal features in a self-supervised or supervised setting. In ECG research, contrastive strategies are still emerging. Kowlgi et al. [21] applied deep learning for PVC-cardiomyopathy detection but did not explore cross-modal contrastive alignment. Wei et al. [17] incorporated contrastive objectives to improve generalization across domains in ECG classification, laying groundwork for further multimodal applications.

MIRAGE advances this line of research by employing a bidirectional contrastive loss to explicitly align representations between ECG time-series and image branches. This facilitates coherent fusion while maintaining modality-specific semantics.

In summary, MIRAGE is inspired by and expands upon recent advances in ECG classification [13–16], structured modeling [18,19], multimodal fusion [1,5,7–10,16,17,20], and contrastive learning [17,21]. It presents a unified, patient-aware framework for robust cardiovascular disease prediction.

## References

[1] Toprak et al., "Diagnostic accuracy of a machine learning algorithm using point-of-care high-sensitivity cardiac troponin assays," 2024.

[2] Kilimci et al., "Heart disease detection using vision-based transformer models from ECG images," 2024.

[3] Tang et al., "Electrocardiogram-language model for few-shot question answering with meta learning," 2024.

[4] Holmstrom et al., "Deep learning-based electrocardiographic screening for chronic kidney disease," 2023.

[5] Kalmady et al., "Development and validation of machine learning algorithms based on electrocardiograms for cardiovascular diseases," 2024.

[6] Khurshid et al., "ECG-based deep learning and clinical risk factors to predict atrial fibrillation," 2022.

[7] Boeddinghaus et al., "Machine learning for myocardial infarction compared with guideline-recommended diagnostic pathways," 2024.

[8] Doudesis et al., "Machine learning for diagnosis of myocardial infarction," 2023.

[9] Al-Zaiti et al., "Machine learning for the ECG diagnosis and risk stratification of occlusion myocardial infarction," 2023.

[10] Al-Zaiti et al., "Machine learning for ECG diagnosis and risk stratification," 2023.

[11] Macfarlane, "New ECG criteria for acute myocardial infarction in patients with left bundle branch block," 2020.

[12] Di Marco et al., "New electrocardiographic algorithm for the diagnosis of acute myocardial infarction in patients with left bundle branch block," 2020.

[13] Liu et al., "A deep learning algorithm for detecting acute myocardial infarction," 2021.

[14] Jafarian et al., "Automating detection and localization of myocardial infarction using deep neural networks," 2020.

[15] Lee et al., "AI-enabled ECG screening of left ventricular ejection fraction," 2022.

[16] Yuan et al., "Deep learning of ECGs in sinus rhythm to predict atrial fibrillation," 2023.

[17] Wei et al., "Bimodal masked autoencoders with internal representation connections for electrocardiogram classification," 2025.

[18] Nie et al., "A time series is worth 64 words: Long-term forecasting with transformers," 2023.

[19] Backhaus et al., "AI-based myocardial strain quantification for risk stratification," 2022.

[20] Lampert et al., "A novel ECG-based deep learning algorithm to predict cardiomyopathy," 2023.

[21] Kowlgi et al., "Deep learning for premature ventricular contraction–cardiomyopathy," 2023.









### B. Time-Series Branch: Graphormer with FiLM

MIRAGE utilizes a Graphormer-based encoder integrated with Feature-wise Linear Modulation (FiLM) to effectively model the structured spatiotemporal characteristics of 12-lead ECG signals. This design explicitly captures the dependencies among leads and incorporates patient-specific clinical context into the representation learning process.

Formally, we define the input ECG signal as $\mathbf{X}_{\text{ECG}} = [\mathbf{x}_1, \dots, \mathbf{x}_{12}] \in \mathbb{R}^{12 \times T}$, where each $\mathbf{x}_\ell \in \mathbb{R}^T$ represents the time series recorded from lead $\ell$. Initially, each lead is independently projected into a latent embedding space via a trainable linear transformation:

$\mathbf{h}_\ell^{(0)} = \mathbf{W}_{\text{proj}} \cdot \mathbf{x}_\ell + \mathbf{b}_{\text{proj}} \in \mathbb{R}^{d}, \quad \forall \ell \in \{1, \dots, 12\}.$

Subsequently, the encoded lead embeddings undergo a series of Graphormer layers designed to explicitly model the spatial relationships between leads. Each Graphormer block updates the node (lead) representations through a graph-based self-attention mechanism that integrates learnable spatial biases. Concretely, at each encoding layer $l$, the attention operation is defined as follows:

$\text{Attn}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{QK}^\top + \mathbf{B}}{\sqrt{d}}\right) \mathbf{V},$

where $\mathbf{B} \in \mathbb{R}^{12 \times 12}$ consists of learnable parameters encoding spatial biases among ECG leads. These biases enable the model to dynamically learn and leverage physiologically meaningful inter-lead relationships.

To further enhance the contextual adaptability of the encoded representations, we incorporate FiLM modulation guided by laboratory-derived biomarkers ($\mathbf{x}_{\text{lab}}$). Specifically, FiLM generates lead-specific scale ($\gamma$) and shift ($\beta$) parameters from the laboratory vector:

$\gamma = f_\gamma(\mathbf{x}_{\text{lab}}), \quad \beta = f_\beta(\mathbf{x}_{\text{lab}}),$

and these parameters modulate the intermediate node features at each encoding layer:

$\mathbf{H}^{(l)}_{\text{FiLM}} = \gamma \odot \mathbf{H}^{(l)} + \beta.$

Through this modulation, the encoder seamlessly integrates patient-specific physiological context, thereby enhancing the discriminative power and personalization of the representations.

After applying multiple Graphormer layers with FiLM modulation, we derive a global embedding by prepending a learnable classification token $\mathbf{h}_{\text{cls}}$. The final global token, representing the entire temporal and spatial ECG context, is obtained through an additional transformer-based projection step:

$\mathbf{z}_{\text{ts,cls}} = \text{TransformerEncoder}([\mathbf{h}_{\text{cls}}; \mathbf{H}^{(L)}]) \in \mathbb{R}^{d}.$

Consequently, this global token encodes temporally rich, spatially informed, and patient-specific features, forming a robust foundation for subsequent multimodal fusion and disease classification tasks within MIRAGE.